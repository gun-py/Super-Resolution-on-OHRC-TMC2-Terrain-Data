{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "thI_IhWS1K6_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import important libraries"
      ],
      "metadata": {
        "id": "_ge15A8Q5YIA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchmetrics\n",
        "!pip install pds4_tools"
      ],
      "metadata": {
        "id": "KnZa6csRITSd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import PIL  #(Pillow)\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from numpy import asarray\n",
        "import pandas as pd\n",
        "import os\n",
        "import cv2\n",
        "import imageio\n",
        "import dlib\n",
        "import seaborn as sns\n",
        "import json\n",
        "import random\n",
        "import torchvision\n",
        "import torchvision.transforms.functional as FT\n",
        "from torchvision import transforms as transforms\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from torch import nn\n",
        "from torch import tensor\n",
        "from torch.utils import data\n",
        "from torch.utils.data import Dataset\n",
        "import torch.backends.cudnn as cudnn\n",
        "from torchmetrics import SpectralAngleMapper as SAM\n",
        "from tqdm import tqdm\n",
        "import math\n",
        "from math import exp, log10, sqrt\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import skimage \n",
        "from skimage.transform import resize\n",
        "from skimage.io import imread, imsave\n",
        "from skimage.color import rgb2gray\n",
        "from skimage.transform import resize,rescale\n",
        "from urllib.request import urlopen\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import glob\n",
        "import pds4_tools\n",
        "from skimage.filters import threshold_otsu\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-02-07T16:40:50.692580Z",
          "iopub.execute_input": "2023-02-07T16:40:50.692923Z",
          "iopub.status.idle": "2023-02-07T16:40:54.657317Z",
          "shell.execute_reply.started": "2023-02-07T16:40:50.692845Z",
          "shell.execute_reply": "2023-02-07T16:40:54.656449Z"
        },
        "trusted": true,
        "id": "uUNJqysH5YIB"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Some constants\n",
        "rgb_weights = torch.FloatTensor([65.481, 128.553, 24.966]).to(device)\n",
        "\n",
        "\n",
        "def convert_image(img, source, target):\n",
        "    \"\"\"\n",
        "    Convert an image from a source format to a target format.\n",
        "    :param img: image\n",
        "    :param source: source format, one of 'pil' (PIL image), '[0, 1]' or '[-1, 1]' (pixel value ranges)\n",
        "    :param target: target format, one of 'pil' (PIL image), '[0, 255]', '[0, 1]', '[-1, 1]' (pixel value ranges),\n",
        "                   'y-channel' (luminance channel Y in the YCbCr color format, used to calculate PSNR and SSIM)\n",
        "    :return: converted image\n",
        "    \"\"\"\n",
        "    assert source in {'pil', '[0, 1]', '[-1, 1]'}, \"Cannot convert from source format %s!\" % source\n",
        "    assert target in {'pil', '[0, 255]', '[0, 1]', '[-1, 1]',\n",
        "                      'y-channel'}, \"Cannot convert to target format %s!\" % target\n",
        "\n",
        "    # Convert from source to [0, 1]\n",
        "    if source == 'pil':\n",
        "        img = FT.to_tensor(img)\n",
        "\n",
        "    elif source == '[0, 1]':\n",
        "        pass  # already in [0, 1]\n",
        "\n",
        "    elif source == '[-1, 1]':\n",
        "        img = (img + 1.) / 2.\n",
        "\n",
        "    # Convert from [0, 1] to target\n",
        "    if target == 'pil':\n",
        "        img = FT.to_pil_image(img)\n",
        "\n",
        "    elif target == '[0, 255]':\n",
        "        img = 255. * img\n",
        "\n",
        "    elif target == '[0, 1]':\n",
        "        pass  # already in [0, 1]\n",
        "\n",
        "    elif target == '[-1, 1]':\n",
        "        img = 2. * img - 1.\n",
        "\n",
        "    elif target == 'y-channel':\n",
        "        img = torch.matmul(255. * img.permute(0, 2, 3, 1)[:, 4:-4, 4:-4, :], rgb_weights) / 255. + 16.\n",
        "\n",
        "    return img"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2023-02-07T16:41:01.245394Z",
          "iopub.execute_input": "2023-02-07T16:41:01.245619Z",
          "iopub.status.idle": "2023-02-07T16:41:04.061797Z",
          "shell.execute_reply.started": "2023-02-07T16:41:01.245592Z",
          "shell.execute_reply": "2023-02-07T16:41:04.061060Z"
        },
        "trusted": true,
        "id": "tZQMQOmB5YID"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convolutional and training model"
      ],
      "metadata": {
        "id": "vgkhY98R5YIE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class ConvolutionalBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    A convolutional block, comprising convolutional, BN, activation layers.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, batch_norm=False, activation=None):\n",
        "        \"\"\"\n",
        "        :param in_channels: number of input channels\n",
        "        :param out_channels: number of output channe;s\n",
        "        :param kernel_size: kernel size\n",
        "        :param stride: stride\n",
        "        :param batch_norm: include a BN layer?\n",
        "        :param activation: Type of activation; None if none\n",
        "        \"\"\"\n",
        "        super(ConvolutionalBlock, self).__init__()\n",
        "\n",
        "        if activation is not None:\n",
        "            activation = activation.lower()\n",
        "            assert activation in {'prelu', 'leakyrelu', 'tanh', 'sigmoid'}\n",
        "\n",
        "        # A container that will hold the layers in this convolutional block\n",
        "        layers = list()\n",
        "\n",
        "        # A convolutional layer\n",
        "        layers.append(\n",
        "            nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride,\n",
        "                      padding=kernel_size // 2))\n",
        "\n",
        "        # A batch normalization (BN) layer, if wanted\n",
        "        if batch_norm is True:\n",
        "            layers.append(nn.BatchNorm2d(num_features=out_channels))\n",
        "\n",
        "        # An activation layer, if wanted\n",
        "        if activation == 'prelu':\n",
        "            layers.append(nn.PReLU())\n",
        "        elif activation == 'leakyrelu':\n",
        "            layers.append(nn.LeakyReLU(0.2))\n",
        "        elif activation == 'tanh':\n",
        "            layers.append(nn.Tanh())\n",
        "        elif activation == 'sigmoid':\n",
        "            layers.append(nn.Sigmoid())\n",
        "\n",
        "        # Put together the convolutional block as a sequence of the layers in this container\n",
        "        self.conv_block = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, input):\n",
        "        \"\"\"\n",
        "        Forward propagation.\n",
        "        :param input: input images, a tensor of size (N, in_channels, w, h)\n",
        "        :return: output images, a tensor of size (N, out_channels, w, h)\n",
        "        \"\"\"\n",
        "        output = self.conv_block(input)  # (N, out_channels, w, h)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "class SubPixelConvolutionalBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    A subpixel convolutional block, comprising convolutional, pixel-shuffle, and PReLU activation layers.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, kernel_size=3, n_channels=64, scaling_factor=2):\n",
        "        \"\"\"\n",
        "        :param kernel_size: kernel size of the convolution\n",
        "        :param n_channels: number of input and output channels\n",
        "        :param scaling_factor: factor to scale input images by (along both dimensions)\n",
        "        \"\"\"\n",
        "        super(SubPixelConvolutionalBlock, self).__init__()\n",
        "\n",
        "        # A convolutional layer that increases the number of channels by scaling factor^2, followed by pixel shuffle and PReLU\n",
        "        self.conv = nn.Conv2d(in_channels=n_channels, out_channels=n_channels * (scaling_factor ** 2),\n",
        "                              kernel_size=kernel_size, padding=kernel_size // 2)\n",
        "        # These additional channels are shuffled to form additional pixels, upscaling each dimension by the scaling factor\n",
        "        self.pixel_shuffle = nn.PixelShuffle(upscale_factor=scaling_factor)\n",
        "        self.prelu = nn.PReLU()\n",
        "\n",
        "    def forward(self, input):\n",
        "        \"\"\"\n",
        "        Forward propagation.\n",
        "        :param input: input images, a tensor of size (N, n_channels, w, h)\n",
        "        :return: scaled output images, a tensor of size (N, n_channels, w * scaling factor, h * scaling factor)\n",
        "        \"\"\"\n",
        "        output = self.conv(input)\n",
        "        output = self.pixel_shuffle(output)\n",
        "        output = self.prelu(output) \n",
        "\n",
        "        return output\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-02-07T16:41:04.077501Z",
          "iopub.execute_input": "2023-02-07T16:41:04.078083Z",
          "iopub.status.idle": "2023-02-07T16:41:04.094273Z",
          "shell.execute_reply.started": "2023-02-07T16:41:04.078044Z",
          "shell.execute_reply": "2023-02-07T16:41:04.093492Z"
        },
        "trusted": true,
        "id": "Zwx26QlF5YIE"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Class SRResNet"
      ],
      "metadata": {
        "id": "DgRejAz35YIF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    A residual block, comprising two convolutional blocks with a residual connection across them.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, kernel_size=3, n_channels=64):\n",
        "        \"\"\"\n",
        "        :param kernel_size: kernel size\n",
        "        :param n_channels: number of input and output channels (same because the input must be added to the output)\n",
        "        \"\"\"\n",
        "        super(ResidualBlock, self).__init__()\n",
        "\n",
        "        # The first convolutional block\n",
        "        self.conv_block1 = ConvolutionalBlock(in_channels=n_channels, out_channels=n_channels, kernel_size=kernel_size,\n",
        "                                              batch_norm=True, activation='PReLu')\n",
        "\n",
        "        # The second convolutional block\n",
        "        self.conv_block2 = ConvolutionalBlock(in_channels=n_channels, out_channels=n_channels, kernel_size=kernel_size,\n",
        "                                              batch_norm=True, activation=None)\n",
        "\n",
        "    def forward(self, input):\n",
        "        \"\"\"\n",
        "        Forward propagation.\n",
        "        :param input: input images, a tensor of size (N, n_channels, w, h)\n",
        "        :return: output images, a tensor of size (N, n_channels, w, h)\n",
        "        \"\"\"\n",
        "        residual = input  # (N, n_channels, w, h)\n",
        "        output = self.conv_block1(input)  # (N, n_channels, w, h)\n",
        "        output = self.conv_block2(output)  # (N, n_channels, w, h)\n",
        "        output = output + residual  # (N, n_channels, w, h)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "class SRResNet(nn.Module):\n",
        "    \"\"\"\n",
        "    The SRResNet, as defined in the paper.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, large_kernel_size=9, small_kernel_size=3, n_channels=64, n_blocks=16, scaling_factor=4):\n",
        "        \"\"\"\n",
        "        :param large_kernel_size: kernel size of the first and last convolutions which transform the inputs and outputs\n",
        "        :param small_kernel_size: kernel size of all convolutions in-between, i.e. those in the residual and subpixel convolutional blocks\n",
        "        :param n_channels: number of channels in-between, i.e. the input and output channels for the residual and subpixel convolutional blocks\n",
        "        :param n_blocks: number of residual blocks\n",
        "        :param scaling_factor: factor to scale input images by (along both dimensions) in the subpixel convolutional block\n",
        "        \"\"\"\n",
        "        super(SRResNet, self).__init__()\n",
        "\n",
        "        # Scaling factor must be 2, 4, or 8\n",
        "        scaling_factor = int(scaling_factor)\n",
        "        assert scaling_factor in {2, 4, 8}, \"The scaling factor must be 2, 4, or 8!\"\n",
        "\n",
        "        # The first convolutional block\n",
        "        self.conv_block1 = ConvolutionalBlock(in_channels=3, out_channels=n_channels, kernel_size=large_kernel_size,\n",
        "                                              batch_norm=False, activation='PReLu')\n",
        "\n",
        "        # A sequence of n_blocks residual blocks, each containing a skip-connection across the block\n",
        "        self.residual_blocks = nn.Sequential(\n",
        "            *[ResidualBlock(kernel_size=small_kernel_size, n_channels=n_channels) for i in range(n_blocks)])\n",
        "\n",
        "        # Another convolutional block\n",
        "        self.conv_block2 = ConvolutionalBlock(in_channels=n_channels, out_channels=n_channels,\n",
        "                                              kernel_size=small_kernel_size,\n",
        "                                              batch_norm=True, activation=None)\n",
        "\n",
        "        # Upscaling is done by sub-pixel convolution, with each such block upscaling by a factor of 2\n",
        "        n_subpixel_convolution_blocks = int(math.log2(scaling_factor))\n",
        "        self.subpixel_convolutional_blocks = nn.Sequential(\n",
        "            *[SubPixelConvolutionalBlock(kernel_size=small_kernel_size, n_channels=n_channels, scaling_factor=2) for i\n",
        "              in range(n_subpixel_convolution_blocks)])\n",
        "\n",
        "        # The last convolutional block\n",
        "        self.conv_block3 = ConvolutionalBlock(\n",
        "            in_channels=n_channels, \n",
        "            out_channels=3, \n",
        "            kernel_size=large_kernel_size,\n",
        "            batch_norm=False, \n",
        "            activation='tanh')\n",
        "\n",
        "    def forward(self, lr_imgs):\n",
        "        \"\"\"\n",
        "        Forward prop.\n",
        "        :param lr_imgs: low-resolution input images, a tensor of size (N, 3, w, h)\n",
        "        :return: super-resolution output images, a tensor of size (N, 3, w * scaling factor, h * scaling factor)\n",
        "        \"\"\"\n",
        "        output = self.conv_block1(lr_imgs)  # (N, 3, w, h)\n",
        "        residual = output  # (N, n_channels, w, h)\n",
        "        output = self.residual_blocks(output)  # (N, n_channels, w, h)\n",
        "        output = self.conv_block2(output)  # (N, n_channels, w, h)\n",
        "        output = output + residual  # (N, n_channels, w, h)\n",
        "        output = self.subpixel_convolutional_blocks(output)  # (N, n_channels, w * scaling factor, h * scaling factor)\n",
        "        sr_imgs = self.conv_block3(output)  # (N, 3, w * scaling factor, h * scaling factor)\n",
        "\n",
        "        return sr_imgs"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-02-07T16:41:04.097117Z",
          "iopub.execute_input": "2023-02-07T16:41:04.097320Z",
          "iopub.status.idle": "2023-02-07T16:41:04.112477Z",
          "shell.execute_reply.started": "2023-02-07T16:41:04.097287Z",
          "shell.execute_reply": "2023-02-07T16:41:04.111744Z"
        },
        "trusted": true,
        "id": "zEJ2bBjm5YIF"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference"
      ],
      "metadata": {
        "id": "14Bndehf5YIF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "crop_size = 128"
      ],
      "metadata": {
        "id": "1uNK24amE1cY"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "EEQVEix-QgSF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xml_path = '/content/drive/MyDrive/TMC_Files/Tmc_data/ch2_tmc_nca_20191228T0043573692_d_img_d18/data/calibrated/20191228/ch2_tmc_nca_20191228T0043573692_d_img_d18.xml'\n",
        "structures = pds4_tools.read(xml_path)  \n",
        "data = structures[0].data\n",
        "data = np.array(data[:(data.shape[0] - data.shape[0]%crop_size), :(data.shape[1] - data.shape[1]%crop_size)])"
      ],
      "metadata": {
        "id": "oH70rTSQqZpR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists('/temp/'):\n",
        "  os.mkdir('/temp/')"
      ],
      "metadata": {
        "id": "Ovbibll3Cvvu"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weights_path = '/content/drive/MyDrive/PlanB/checkpoint_srresnet_10_epochs_128x128.pth.tar'\n",
        "checkpoint = torch.load(weights_path)\n",
        "model = checkpoint['model']\n",
        "model.to(device)\n",
        "model.eval()\n",
        "print('Model Loaded')"
      ],
      "metadata": {
        "id": "RP8WR-6yDIGZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_rows = data.shape[0]//crop_size\n",
        "num_cols = data.shape[1]//crop_size\n",
        "\n",
        "line_img = None\n",
        "full_img = None\n",
        "\n",
        "for i in tqdm(range(num_rows)):\n",
        "  for j in range(num_cols):\n",
        "\n",
        "    img_crop = data[i*crop_size: (i+1)*crop_size, j*crop_size: (j+1)*crop_size]\n",
        "    plt.imsave('/temp/img_crop.png', img_crop, cmap = 'gray')\n",
        "    img = Image.open('/temp/img_crop.png', mode='r')\n",
        "    img = img.convert('RGB')\n",
        "    img_lr = convert_image(img, source='pil', target='[-1, 1]')\n",
        "    img_sr = model(img_lr.unsqueeze(0).to(device))\n",
        "\n",
        "\n",
        "    if line_img is not None:\n",
        "      line_img = np.concatenate((line_img, img_sr.detach().cpu().squeeze().numpy().transpose(1, 2, 0)*0.5 + 0.5), axis=1)\n",
        "    else:\n",
        "      line_img = img_sr.detach().cpu().squeeze().numpy().transpose(1, 2, 0)*0.5 + 0.5\n",
        "    \n",
        "    os.remove('/temp/img_crop.png')\n",
        "    del img_sr\n",
        "\n",
        "    im = Image.fromarray(line_img.astype('uint8'))\n",
        "    im.save(f'/content/drive/MyDrive/PlanB/Inference/row_{i}.tif')\n",
        "\n",
        "    gc.collect()"
      ],
      "metadata": {
        "id": "haz4UnAoqukb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_image = None\n",
        "\n",
        "path = '/content/drive/MyDrive/PlanB/Inference'\n",
        "for file in os.listdir(path):\n",
        "  im = Image.open(path+file)\n",
        "  line_img = np.array(im)\n",
        "\n",
        "  if full_image is not None:\n",
        "    full_img = np.concatenate((full_img, line_img), axis=0)\n",
        "  else:\n",
        "    full_image = line_img\n",
        "  \n",
        "  im = Image.fromarray(full_img.astype('uint8'))\n",
        "  im.save(f'/content/drive/MyDrive/PlanB/full_img.tif')"
      ],
      "metadata": {
        "id": "pkR15KdZ-1TP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (10,100))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(data, cmap='gray')\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(full_img, cmap='gray')"
      ],
      "metadata": {
        "id": "bsTNTdBeDjGt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}